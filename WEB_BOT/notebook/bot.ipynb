{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e2328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "from fastapi import FastAPI, Request\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain + LangGraph imports\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.document_loaders import JSONLoader, PyPDFLoader\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "GEMINI_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "chat_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    api_key=GEMINI_KEY,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "class FileMemory:\n",
    "    def __init__(self, user: str = \"default\"):\n",
    "        self.path = Path(f\"{user}_session.pkl\")\n",
    "        self.buffer = ConversationBufferMemory(return_messages=True)\n",
    "        if self.path.exists():\n",
    "            try:\n",
    "                self.buffer = pickle.load(open(self.path, \"rb\"))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def save(self):\n",
    "        pickle.dump(self.buffer, open(self.path, \"wb\"))\n",
    "\n",
    "\n",
    "class SessionState(BaseModel):\n",
    "    user_text: str = \"\"\n",
    "    user_id: str = \"anon\"\n",
    "    memory: Any = None\n",
    "    reply: str | None = None\n",
    "\n",
    "    vax_file: str = \"data/vaccination_schedule.json\"\n",
    "    outbreak_file: str = \"data/outbreak_latest.pdf\"\n",
    "    index_folder: str = \"data/faiss_idx\"\n",
    "\n",
    "    vax_data: Any = None\n",
    "    outbreak_chunks: Any = None\n",
    "    faiss_store: Any = None\n",
    "\n",
    "\n",
    "def intake(state: SessionState) -> SessionState:\n",
    "    return state\n",
    "\n",
    "def fetch_vaccines(state: SessionState) -> SessionState:\n",
    "    try:\n",
    "        state.vax_data = JSONLoader(file_path=state.vax_file).load()\n",
    "    except:\n",
    "        state.vax_data = None\n",
    "    return state\n",
    "\n",
    "def fetch_outbreak(state: SessionState) -> SessionState:\n",
    "    try:\n",
    "        state.outbreak_chunks = PyPDFLoader(state.outbreak_file).load_and_split()\n",
    "    except:\n",
    "        state.outbreak_chunks = None\n",
    "    return state\n",
    "\n",
    "def build_index(state: SessionState) -> SessionState:\n",
    "    if not state.outbreak_chunks:\n",
    "        return state\n",
    "    embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    faiss_db = FAISS.from_documents(state.outbreak_chunks, embedder)\n",
    "    faiss_db.save_local(state.index_folder)\n",
    "    state.faiss_store = faiss_db\n",
    "    return state\n",
    "\n",
    "def classify_and_reply(state: SessionState) -> SessionState:\n",
    "    msg = state.user_text.lower()\n",
    "\n",
    "    if \"outbreak\" in msg:\n",
    "        state.reply = outbreak_reply(state)\n",
    "    elif \"vaccine\" in msg or \"schedule\" in msg:\n",
    "        state.reply = vaccine_reply(state)\n",
    "    else:\n",
    "        state.reply = generic_reply(state)\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def outbreak_reply(state: SessionState) -> str:\n",
    "    if not state.faiss_store:\n",
    "        return \"Outbreak data not yet available.\"\n",
    "\n",
    "    retriever = state.faiss_store.as_retriever(search_kwargs={\"k\": 4})\n",
    "    convo = ConversationalRetrievalChain.from_llm(\n",
    "        llm=chat_model,\n",
    "        retriever=retriever,\n",
    "        memory=state.memory.buffer if state.memory else None\n",
    "    )\n",
    "    answer = convo.run(state.user_text)\n",
    "    if state.memory:\n",
    "        state.memory.save()\n",
    "    return answer\n",
    "\n",
    "def vaccine_reply(state: SessionState) -> str:\n",
    "    try:\n",
    "        schedule = json.load(open(state.vax_file, \"r\", encoding=\"utf-8\"))\n",
    "        sched_str = json.dumps(schedule, indent=2)\n",
    "    except:\n",
    "        return \"Could not load vaccination data.\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a health assistant. \n",
    "    Use the vaccination schedule below to answer the question.\n",
    "\n",
    "    VACCINATION DATA:\n",
    "    {sched_str}\n",
    "\n",
    "    QUESTION:\n",
    "    {state.user_text}\n",
    "    \"\"\"\n",
    "    return chat_model.invoke(prompt).content\n",
    "\n",
    "def generic_reply(state: SessionState) -> str:\n",
    "    return chat_model.invoke([{\"role\": \"user\", \"content\": state.user_text}]).content\n",
    "\n",
    "\n",
    "graph = StateGraph(SessionState)\n",
    "graph.add_node(\"intake\", intake)\n",
    "graph.add_node(\"vaccines\", fetch_vaccines)\n",
    "graph.add_node(\"outbreaks\", fetch_outbreak)\n",
    "graph.add_node(\"index\", build_index)\n",
    "graph.add_node(\"decide\", classify_and_reply)\n",
    "\n",
    "graph.add_edge(START, \"intake\")\n",
    "graph.add_edge(\"intake\", \"decide\")\n",
    "graph.add_edge(\"vaccines\", \"index\")\n",
    "graph.add_edge(\"outbreaks\", \"index\")\n",
    "graph.add_edge(\"index\", \"decide\")\n",
    "graph.add_edge(\"decide\", END)\n",
    "\n",
    "app_graph = graph.compile()\n",
    "\n",
    "\n",
    "api = FastAPI()\n",
    "\n",
    "@api.post(\"/chat\")\n",
    "async def chat(req: Request):\n",
    "    payload = await req.json()\n",
    "    user_msg = payload.get(\"message\", \"\")\n",
    "    user_id = payload.get(\"user_id\", \"anon\")\n",
    "\n",
    "    mem = FileMemory(user_id)\n",
    "    state = SessionState(user_text=user_msg, user_id=user_id, memory=mem)\n",
    "\n",
    "    result = app_graph.invoke(state)\n",
    "    return {\"response\": result.reply}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31520898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HealthGraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
